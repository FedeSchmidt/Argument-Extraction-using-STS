{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c9cb4-8042-4c27-9142-b95367bb17d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import utils\n",
    "import categorization\n",
    "import scoring\n",
    "\n",
    "importlib.reload(categorization)\n",
    "importlib.reload(scoring)\n",
    "\n",
    "from utils import *\n",
    "from categorization import *\n",
    "from scoring import get_N_score\n",
    "\n",
    "current_path = pathlib.Path().resolve().parent\n",
    "print(current_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f64b6-8cec-4793-bd65-caa5382f1825",
   "metadata": {},
   "source": [
    "## Categorization of predicted arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb1f24-75e0-413a-b646-60e27f251d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_PM = 1\n",
    "lambda_MU = 1\n",
    "lambda_UNR = 1\n",
    "\n",
    "def get_line_of_info(dataname, sts_type, tanda, criterion = 'best_f1'):\n",
    "    best_f1 = 0\n",
    "    best_model = 0\n",
    "    best_predictions = None\n",
    "    data1 = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{i}.txt')\n",
    "        tokens, labels, predictions = parse_file(dirpath)\n",
    "        run_f1 = f1_score(labels, predictions, average='macro')\n",
    "        if (run_f1 > best_f1):\n",
    "            best_model = i\n",
    "            best_f1 = run_f1\n",
    "    \n",
    "        instances = parse_file_for_arg_level(dirpath)\n",
    "        id_values_pairs = get_values_for_predicted_arguments(instances)\n",
    "\n",
    "        number_match, number_partial_match, number_made_ups = get_number_predicted_arguments_per_category(id_values_pairs)\n",
    "        number_unrecognized = get_unrecognized_arguments(instances)\n",
    "        \n",
    "        number_golds = get_gold_arguments(instances)\n",
    "        number_preds = get_pred_arguments(instances)\n",
    "        \n",
    "        assert (number_match + number_partial_match + number_made_ups) == number_preds\n",
    "\n",
    "        r_values_for_no_match = get_partial_match_values(id_values_pairs)\n",
    "                \n",
    "        # N_score = get_N_score(number_match, number_partial_match, number_made_ups, number_unrecognized, r_values_for_no_match, number_golds, number_preds, \n",
    "                             # lambda_PM, lambda_MU, lambda_UNR)\n",
    "       \n",
    "        data1.append([number_match, number_partial_match, number_made_ups, number_unrecognized])\n",
    "\n",
    "    stats_of_best_model = data1[best_model]\n",
    "\n",
    "    averages = []\n",
    "    for i in range(len(data1[0])):\n",
    "        averages.append(round(np.mean([item[i] for item in data1])))    \n",
    "        # if i == len(data1[0])-1:\n",
    "        #     averages.append(round(np.mean([item[i] for item in data1]), 3))\n",
    "        # else:\n",
    "        #     averages.append(round(np.mean([item[i] for item in data1])))    \n",
    "    \n",
    "    s = \"\"\n",
    "    for l1, l2 in zip(stats_of_best_model, averages):\n",
    "        s += f\" & {l1} ({l2})\"\n",
    "\n",
    "    return s\n",
    "\n",
    "# , round(N_score, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf5dc8-8dab-4ba2-9af0-03301325eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataname in ['ugen', 'pe', 'webis']:\n",
    "    sts_type = 'no_sts'\n",
    "    print(dataname, '-', sts_type)\n",
    "    info = get_line_of_info(dataname, sts_type, 'simple')\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc6c10-ba4a-4504-943d-41a4ced5b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataname in ['ugen', 'pe', 'webis']:\n",
    "    # 'ugen',\n",
    "    for sts_type in ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft']:\n",
    "        print(dataname, '-', sts_type)\n",
    "        info = get_line_of_info(dataname, sts_type, 'tanda-2')\n",
    "        print(info)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f221264-dd24-4ebb-bbce-9b2ed5ed5c7b",
   "metadata": {},
   "source": [
    "## N-score (syntactic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b2abf-073d-461d-a2ca-54721d1c3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'ugen'\n",
    "tanda = 'simple'\n",
    "sts_type = 'no_sts'\n",
    "best_f1 = 0\n",
    "best_model = 0\n",
    "\n",
    "lambda_PM = 1\n",
    "lambda_MU = 1\n",
    "lambda_UNR = 1\n",
    "\n",
    "N_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{i}.txt')\n",
    "    tokens, labels, predictions = parse_file(dirpath)\n",
    "    run_f1 = f1_score(labels, predictions, average='macro')\n",
    "    if (run_f1 > best_f1):\n",
    "        best_model = i\n",
    "        best_f1 = run_f1\n",
    "\n",
    "    instances = parse_file_for_arg_level(dirpath)\n",
    "    id_values_pairs = get_values_for_predicted_arguments(instances)\n",
    "    number_match, number_partial_match, number_made_ups = get_number_predicted_arguments_per_category(id_values_pairs)\n",
    "    number_unrecognized = get_unrecognized_arguments(instances)\n",
    "        \n",
    "    number_golds = get_gold_arguments(instances)\n",
    "    number_preds = get_pred_arguments(instances)\n",
    "        \n",
    "    assert (number_match + number_partial_match + number_made_ups) == number_preds\n",
    "\n",
    "    r_values_for_no_match = get_partial_match_values(id_values_pairs)\n",
    "    \n",
    "    N_score = get_N_score(number_match, number_partial_match, number_made_ups, number_unrecognized, r_values_for_no_match, \n",
    "                          number_golds, number_preds, \n",
    "                          lambda_PM, lambda_MU, lambda_UNR)\n",
    "\n",
    "    N_scores.append(N_score)\n",
    "\n",
    "\n",
    "print(\"BEST score: \", round(N_scores[best_model], 3))\n",
    "\n",
    "print(\"AVG score: \", round(np.mean(N_scores), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64694f-a963-4c41-bc97-51fd81cd6d2c",
   "metadata": {},
   "source": [
    "### Ploting syntactic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d0e10-8794-4172-a08f-ea9663f0b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_ploting(dataname, sts_types, tandas):\n",
    "    data_to_plot = []\n",
    "\n",
    "    for sts_type, tanda in zip(sts_types, tandas):\n",
    "        best_f1 = 0\n",
    "        best_model = 0 \n",
    "        \n",
    "        for i in range(10):\n",
    "            dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{i}.txt')\n",
    "            tokens, labels, predictions = parse_file(dirpath)\n",
    "            run_f1 = f1_score(labels, predictions, average='macro')\n",
    "            if (run_f1 > best_f1):\n",
    "                best_model = i\n",
    "                best_f1 = run_f1\n",
    "        \n",
    "        dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{best_model}.txt')\n",
    "        instances = parse_file_for_arg_level(dirpath)\n",
    "        id_values_pairs = get_values_for_predicted_arguments(instances)\n",
    "        \n",
    "        number_match, number_partial_match, number_made_ups = get_number_predicted_arguments_per_category(id_values_pairs)\n",
    "        number_unrecognized = get_unrecognized_arguments(instances)\n",
    "                \n",
    "        number_golds = get_gold_arguments(instances)\n",
    "        number_preds = get_pred_arguments(instances)\n",
    "                \n",
    "        assert (number_match + number_partial_match + number_made_ups) == number_preds\n",
    "            \n",
    "        data1 = get_partial_match_values(id_values_pairs)\n",
    "        data_to_plot.append(data1)\n",
    "\n",
    "    return data_to_plot\n",
    "\n",
    "def get_data_for_ploting_all_models(dataname, sts_types, tandas):\n",
    "    data_to_plot = []\n",
    "\n",
    "    for sts_type, tanda in zip(sts_types, tandas):\n",
    "        data1 = []\n",
    "        for i in range(10):       \n",
    "            dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{i}.txt')\n",
    "            instances = parse_file_for_arg_level(dirpath)\n",
    "            id_values_pairs = get_values_for_predicted_arguments(instances)\n",
    "            \n",
    "            number_match, number_partial_match, number_made_ups = get_number_predicted_arguments_per_category(id_values_pairs)\n",
    "            number_unrecognized = get_unrecognized_arguments(instances)\n",
    "                    \n",
    "            number_golds = get_gold_arguments(instances)\n",
    "            number_preds = get_pred_arguments(instances)\n",
    "                    \n",
    "            assert (number_match + number_partial_match + number_made_ups) == number_preds\n",
    "                \n",
    "            data1 += get_partial_match_values(id_values_pairs)\n",
    "        data_to_plot.append(data1)\n",
    "\n",
    "    return data_to_plot\n",
    "\n",
    "def get_percentages(data_to_plot, numbins):\n",
    "    percentages = []\n",
    "    num_bins = numbins\n",
    "    for values in data_to_plot:\n",
    "        # Define the number of bins\n",
    "        bins = np.linspace(0, 1, num_bins + 1)\n",
    "        \n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "        total_count = len(values)\n",
    "        percentages.append((counts / total_count) * 100)\n",
    "    \n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f25e7-da4d-4b38-b09a-dc3d88ff8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanames = ['ugen', 'pe', 'webis']\n",
    "map_datanames = {'ugen': 'UGEN', 'webis':'WE', 'pe': 'AAEC'}\n",
    "\n",
    "sts_types = ['no_sts'] + ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft'] #  ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft']:\n",
    "tandas = ['simple'] + ['tanda-2']*3\n",
    "\n",
    "# Create a heatmap\n",
    "fig, axis = plt.subplots(1, 3, figsize=(16, 4), sharey=True)\n",
    "\n",
    "for index_axis, dataname in enumerate(datanames):\n",
    "\n",
    "    data_to_plot = get_data_for_ploting(dataname, sts_types, tandas)\n",
    "    # data_to_plot = get_data_for_ploting_all_models(dataname, sts_types, tandas)\n",
    "    \n",
    "    percentages = get_percentages(data_to_plot, numbins = 4)\n",
    "\n",
    "    rounded_percentages = [np.array([round(x) for x in p]) for p in percentages]\n",
    "    # print(percentages)\n",
    "    # print(rounded_percentages)\n",
    "\n",
    "    for row_percentajes in rounded_percentages:\n",
    "        difference = 100 - sum(row_percentajes)\n",
    "        \n",
    "        for i in range(abs(difference)):\n",
    "            index = i % len(row_percentajes)\n",
    "            if difference > 0:\n",
    "                row_percentajes[index] += 1\n",
    "            elif difference < 0:\n",
    "                row_percentajes[index] -= 1\n",
    "\n",
    "    # print(rounded_percentages)\n",
    "    percentages = rounded_percentages\n",
    "\n",
    "    data = np.array([percentages[0], percentages[1], percentages[2], percentages[3]])\n",
    "    \n",
    "    xtickslabels = ['0 - 0.25', '0.25 - 0.5', '0.5 - 0.75', '0.75 - 1']\n",
    "    yticklabels=[\"Simple\", \"SBERT\", \"SBERT\"+\"\\n\"+\"[STSb+BWS]\", \"argueBERT\"+\"\\n\"+\"[STSb+BWS]\"]\n",
    "    \n",
    "    def fmt(x, pos):\n",
    "        return f'{round(x)}%'\n",
    "    \n",
    "    ax = axis[index_axis]\n",
    "    sns.heatmap(data, annot=True, fmt=\"\", cbar=False,\n",
    "                cmap=\"Blues\", xticklabels=xtickslabels, \n",
    "                yticklabels=yticklabels, ax=ax, annot_kws={\"size\": 18})\n",
    "    \n",
    "    # Format the annotations with percentage symbol\n",
    "    for text in ax.texts:\n",
    "        text.set_text(fmt(float(text.get_text()), None))\n",
    "    \n",
    "    # Rotate y-tick labels\n",
    "    # ax.set_yticks(yticklabels)\n",
    "    ax.set_yticklabels(yticklabels, rotation=0, fontsize = 11)\n",
    "        \n",
    "    # Add labels\n",
    "    ax.set_xlabel('R', fontsize = 11)\n",
    "    # ax.set_ylabel('Modelos')\n",
    "    \n",
    "    ax.set_title(map_datanames[dataname])\n",
    "    \n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(current_path / 'figures' / 'syntactic_simil_all_models.pdf')\n",
    "plt.savefig(current_path / 'figures' / 'syntactic_simil_best_models.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798c654-5225-4f6a-b52b-651b2de8ff95",
   "metadata": {},
   "source": [
    "## N-score (semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485046d-6a12-4299-ba62-0f54f05c96ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'pe'\n",
    "# tanda = 'simple'\n",
    "tanda = 'tanda-2'\n",
    "# sts_type = 'no_sts'\n",
    "# sts_type = 'sts_sbert'\n",
    "# sts_type = 'sts_sbert_ft'\n",
    "sts_type = 'sts_arguebert_ft'\n",
    "best_f1 = 0\n",
    "best_model = 0\n",
    "\n",
    "lambda_PM = 1\n",
    "lambda_MU = 1\n",
    "lambda_UNR = 1\n",
    "\n",
    "N_scores = []\n",
    "\n",
    "semModel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{i}.txt')\n",
    "    tokens, labels, predictions = parse_file(dirpath)\n",
    "    run_f1 = f1_score(labels, predictions, average='macro')\n",
    "    if (run_f1 > best_f1):\n",
    "        best_model = i\n",
    "        best_f1 = run_f1\n",
    "\n",
    "    instances = parse_file_for_arg_level(dirpath)\n",
    "    # id_values_pairs = get_values_for_predicted_arguments(instances)\n",
    "    id_values_pairs = get_semantic_values_for_predicted_arguments(instances, semModel)\n",
    "\n",
    "    # print(len(id_values_pairs), id_values_pairs)\n",
    "    number_match, number_partial_match, number_made_ups = get_number_predicted_arguments_per_category(id_values_pairs)\n",
    "    number_unrecognized = get_unrecognized_arguments(instances)\n",
    "        \n",
    "    number_golds = get_gold_arguments(instances)\n",
    "    number_preds = get_pred_arguments(instances)\n",
    "        \n",
    "    assert (number_match + number_partial_match + number_made_ups) == number_preds\n",
    "\n",
    "    # r_values_for_no_match = get_partial_match_values(id_values_pairs)\n",
    "\n",
    "    # print(len(r_values_for_no_match), r_values_for_no_match)\n",
    "    semantic_similarity_values_for_no_match = get_partial_match_values(id_values_pairs)\n",
    "    \n",
    "    N_score = get_N_score(number_match, number_partial_match, number_made_ups, number_unrecognized, semantic_similarity_values_for_no_match, \n",
    "                          number_golds, number_preds, \n",
    "                          lambda_PM, lambda_MU, lambda_UNR)\n",
    "\n",
    "    print(N_score)\n",
    "    # break\n",
    "\n",
    "    N_scores.append(N_score)\n",
    "\n",
    "\n",
    "print(\"BEST score: \", round(N_scores[best_model], 3))\n",
    "\n",
    "print(\"AVG score: \", round(np.mean(N_scores), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de82d1d-44b9-4457-b678-f58a1e54ca0a",
   "metadata": {},
   "source": [
    "### Ploting semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f5433-7b51-41c5-a84e-1c1283f1e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_ploting(dataname, sts_types, tandas, semModel):\n",
    "    data_to_plot = []\n",
    "\n",
    "    for sts_type, tanda in zip(sts_types, tandas):\n",
    "        best_f1 = 0\n",
    "        best_model = 0 \n",
    "        \n",
    "        for i in range(10):\n",
    "            dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{i}.txt')\n",
    "            tokens, labels, predictions = parse_file(dirpath)\n",
    "            run_f1 = f1_score(labels, predictions, average='macro')\n",
    "            if (run_f1 > best_f1):\n",
    "                best_model = i\n",
    "                best_f1 = run_f1\n",
    "        \n",
    "        dirpath = str(current_path / f'results-{tanda}' / f'{dataname}_{sts_type}' / f'test_{dataname}_{best_model}.txt')\n",
    "        instances = parse_file_for_arg_level(dirpath)\n",
    "        id_values_pairs = get_semantic_values_for_predicted_arguments(instances, semModel)\n",
    "        \n",
    "        number_match, number_partial_match, number_made_ups = get_number_predicted_arguments_per_category(id_values_pairs)\n",
    "        number_unrecognized = get_unrecognized_arguments(instances)\n",
    "                \n",
    "        number_golds = get_gold_arguments(instances)\n",
    "        number_preds = get_pred_arguments(instances)\n",
    "                \n",
    "        assert (number_match + number_partial_match + number_made_ups) == number_preds\n",
    "            \n",
    "        data1 = get_partial_match_values(id_values_pairs)\n",
    "        data_to_plot.append(data1)\n",
    "\n",
    "    return data_to_plot\n",
    "\n",
    "\n",
    "def get_percentages(data_to_plot, numbins):\n",
    "    percentages = []\n",
    "    num_bins = numbins\n",
    "    for values in data_to_plot:\n",
    "        # Define the number of bins\n",
    "        bins = np.linspace(0, 1, num_bins + 1)\n",
    "        \n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "        total_count = len(values)\n",
    "        percentages.append((counts / total_count) * 100)\n",
    "    \n",
    "    return percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8aad7e-89cb-49fc-bdb5-7bf6a3af23e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanames = ['ugen', 'pe', 'webis']\n",
    "map_datanames = {'ugen': 'UGEN', 'webis':'WE', 'pe': 'AAEC'}\n",
    "\n",
    "sts_types = ['no_sts'] + ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft'] #  ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft']:\n",
    "tandas = ['simple'] + ['tanda-2']*3\n",
    "\n",
    "# semModel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# semModel = SentenceTransformer(str(current_path / 'similarity_models' / 'sbert-all-MiniLM-L6-v2-sts-bws' / 'final'))\n",
    "semModel = SentenceTransformer(str(current_path / 'similarity_models' / 'argueBert_base_similar-sts-bws' / 'final'))\n",
    "\n",
    "# Create a heatmap\n",
    "fig, axis = plt.subplots(1, 3, figsize=(16, 4), sharey=True)\n",
    "\n",
    "for index_axis, dataname in enumerate(datanames):\n",
    "\n",
    "    data_to_plot = get_data_for_ploting(dataname, sts_types, tandas, semModel)\n",
    "    \n",
    "    percentages = get_percentages(data_to_plot, numbins = 4)\n",
    "\n",
    "    rounded_percentages = [np.array([round(x) for x in p]) for p in percentages]\n",
    "\n",
    "    for row_percentajes in rounded_percentages:\n",
    "        difference = 100 - sum(row_percentajes)\n",
    "        \n",
    "        for i in range(abs(difference)):\n",
    "            index = i % len(row_percentajes)\n",
    "            if difference > 0:\n",
    "                row_percentajes[index] += 1\n",
    "            elif difference < 0:\n",
    "                row_percentajes[index] -= 1\n",
    "\n",
    "    percentages = rounded_percentages\n",
    "\n",
    "    data = np.array([percentages[0], percentages[1], percentages[2], percentages[3]])\n",
    "    \n",
    "    xtickslabels = ['0 - 0.25', '0.25 - 0.5', '0.5 - 0.75', '0.75 - 1']\n",
    "    yticklabels=[\"Simple\", \"SBERT\", \"SBERT\"+\"\\n\"+\"[STSb+BWS]\", \"argueBERT\"+\"\\n\"+\"[STSb+BWS]\"]\n",
    "    \n",
    "    def fmt(x, pos):\n",
    "        return f'{round(x)}%'\n",
    "    \n",
    "    ax = axis[index_axis]\n",
    "    sns.heatmap(data, annot=True, fmt=\"\", cbar=False,\n",
    "                cmap=\"Blues\", xticklabels=xtickslabels, \n",
    "                yticklabels=yticklabels, ax=ax, annot_kws={\"size\": 18})\n",
    "    \n",
    "    # Format the annotations with percentage symbol\n",
    "    for text in ax.texts:\n",
    "        text.set_text(fmt(float(text.get_text()), None))\n",
    "    \n",
    "    # Rotate y-tick labels\n",
    "    # ax.set_yticks(yticklabels)\n",
    "    ax.set_yticklabels(yticklabels, rotation=0)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel('R-index')\n",
    "    # ax.set_ylabel('Modelos')\n",
    "    \n",
    "    ax.set_title(map_datanames[dataname])\n",
    "    \n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(current_path / 'figures' / 'semantic_simil_best_models_arguebertft.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4fe7b-1582-43eb-b5d6-63a539445f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanames = ['ugen', 'pe', 'webis']\n",
    "map_datanames = {'ugen': 'UGEN', 'webis':'WE', 'pe': 'AAEC'}\n",
    "\n",
    "sts_types = ['no_sts'] + ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft'] #  ['sts_sbert', 'sts_sbert_ft', 'sts_arguebert_ft']:\n",
    "tandas = ['simple'] + ['tanda-2']*3\n",
    "\n",
    "fig, axis = plt.subplots(1, 3, figsize=(16, 4), sharey=True)\n",
    "\n",
    "\n",
    "# rounded_percentages = []\n",
    "\n",
    "# rounded_percentages.append([\n",
    "#     np.array([37, 11, 28, 24]),\n",
    "#     np.array([1, 18, 23, 58]),\n",
    "#     np.array([1, 21, 22, 56]),\n",
    "#     np.array([1, 20, 22, 57])\n",
    "# ])\n",
    "\n",
    "# rounded_percentages.append([\n",
    "#     np.array([11, 5, 26, 58]),\n",
    "#     np.array([0, 7, 10, 83]),\n",
    "#     np.array([0, 3, 10, 87]),\n",
    "#     np.array([0, 1, 16, 83])\n",
    "# ])\n",
    "\n",
    "# rounded_percentages.append([\n",
    "#     np.array([5, 7, 33, 55]),\n",
    "#     np.array([1, 2, 12, 85]),\n",
    "#     np.array([0, 5, 13, 82]),\n",
    "#     np.array([0, 5, 14, 81])\n",
    "# ])\n",
    "\n",
    "\n",
    "rounded_percentages = []\n",
    "\n",
    "rounded_percentages.append([\n",
    "    np.array([24, 23, 34, 19]),\n",
    "    np.array([6, 13, 12, 69]),\n",
    "    np.array([7, 13, 14, 66]),\n",
    "    np.array([4, 14, 14, 68])\n",
    "])\n",
    "\n",
    "rounded_percentages.append([\n",
    "    np.array([6, 10, 32, 52]),\n",
    "    np.array([2, 6, 13, 79]),\n",
    "    np.array([0, 2, 13, 85]),\n",
    "    np.array([0, 2, 14, 84])\n",
    "])\n",
    "\n",
    "rounded_percentages.append([\n",
    "    np.array([2, 8, 38, 52]),\n",
    "    np.array([1, 1, 11, 87]),\n",
    "    np.array([0, 4, 15, 81]),\n",
    "    np.array([0, 3, 15, 82])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for index_axis, dataname in enumerate(datanames):\n",
    "    # print(rounded_percentages)\n",
    "    percentages = rounded_percentages[index_axis]\n",
    "    \n",
    "    data = np.array([percentages[0], percentages[1], percentages[2], percentages[3]])\n",
    "        \n",
    "    xtickslabels = ['0 - 0.25', '0.25 - 0.5', '0.5 - 0.75', '0.75 - 1']\n",
    "    yticklabels=[\"Simple\", \"SBERT\", \"SBERT\"+\"\\n\"+\"[STSb+BWS]\", \"argueBERT\"+\"\\n\"+\"[STSb+BWS]\"]\n",
    "        \n",
    "    def fmt(x, pos):\n",
    "        return f'{round(x)}%'\n",
    "        \n",
    "    ax = axis[index_axis]\n",
    "    sns.heatmap(data, annot=True, fmt=\"\", cbar=False,\n",
    "                    cmap=\"Blues\", xticklabels=xtickslabels, \n",
    "                    yticklabels=yticklabels, ax=ax, annot_kws={\"size\": 18})\n",
    "        \n",
    "    # Format the annotations with percentage symbol\n",
    "    for text in ax.texts:\n",
    "        text.set_text(fmt(float(text.get_text()), None))\n",
    "        \n",
    "    # Rotate y-tick labels\n",
    "    # ax.set_yticks(yticklabels)\n",
    "    ax.set_yticklabels(yticklabels, rotation=0, fontsize = 11)\n",
    "        \n",
    "    # Add labels\n",
    "    ax.set_xlabel('Argue-BERT [STSb+BWS]', fontsize = 11)\n",
    "    # ax.set_ylabel('Modelos')\n",
    "        \n",
    "    ax.set_title(map_datanames[dataname])\n",
    "    \n",
    "    \n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(current_path / 'figures' / 'semantic_simil_best_models_arguebertft.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61c6d2-d0bd-400c-9311-dcfd5959d9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
