{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8843621,"sourceType":"datasetVersion","datasetId":5117154},{"sourceId":9480167,"sourceType":"datasetVersion","datasetId":5766351}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport random\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')  \nimport torch\nfrom torch import nn, cuda, optim\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport copy\n\n# import transformers\nfrom transformers import (\n    BertModel,\n    BertForTokenClassification,\n    BertTokenizerFast,\n    AutoTokenizer,\n    AutoModelForTokenClassification,\n    get_linear_schedule_with_warmup\n)\n\n! pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    precision_recall_fscore_support,\n    classification_report\n)\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nif device == 'cuda':\n    torch.cuda.empty_cache()\nprint(device)\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef generate_random_seed():\n    return random.randint(1, 1000)\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        \nrs = generate_random_seed()\nset_random_seed(rs)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    def __init__(self, runs, epochs, batch_size, \n                 num_labels = 3, label_list = ['O', 'B', 'I'], model_checkpoint = \"allenai/longformer-base-4096\", max_len = 1024, use_similarity = False, mode_similarity = 'sbert', lambda_madeup = 1, lambda_unrecognized = 1):\n        self.num_labels = num_labels\n        self.label_list = label_list\n        self.model_checkpoint = model_checkpoint\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint, add_prefix_space=True)\n        self.runs = runs\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.max_len = max_len\n        self.use_similarity = use_similarity\n        self.mode_similarity = mode_similarity\n        self.lambda_mu = lambda_madeup\n        self.lambda_unr = lambda_unrecognized\n        self.similarity_model = None\n        if (use_similarity):\n            if mode_similarity == 'sbert':\n                self.similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n            elif mode_similarity == 'sbert-sts-bws':\n                self.similarity_model = SentenceTransformer(\"/kaggle/input/similaritymodels/sbert-sts-bws/content/models/sbert-all-MiniLM-L6-v2-sts-bws/final\")\n            elif mode_similarity == 'arguebert-sts-bws':\n                self.similarity_model = SentenceTransformer(\"/kaggle/input/similaritymodels/arguebert-sts-bws/content/models/argueBert_base_similar-sts-bws/final\")\n            else:\n                raise Exception(\"Incorrect similarity mode. Options are: 'sbert', 'sbert-sts-bws' and 'arguebert-sts-bws'.\")\n                \nclass BestModelsTracker:\n    def __init__(self):\n        self.best_f1 = -float('inf')\n        self.best_loss = float('inf')\n        self.best_pm = -float('inf')\n        self.best_f1_model = None\n        self.best_loss_model = None\n        self.best_pm_model = None\n        self.best_run_f1 = None\n        self.best_run_loss = None\n        self.best_run_pm = None\n        self.best_epoch_f1 = None\n        self.best_epoch_loss = None\n        self.best_epoch_pm = None\n\n    def update(self, model, val_loss, macro_f1, pm, run, epoch):\n        # Update best F1 model\n        if macro_f1 > self.best_f1:\n            self.best_f1 = macro_f1\n            self.best_run_f1 = run\n            self.best_epoch_f1 = epoch\n            self.best_f1_model = copy.deepcopy(model.state_dict())\n\n        # Update best loss model\n        if val_loss < self.best_loss:\n            self.best_loss = val_loss\n            self.best_run_loss = run\n            self.best_epoch_loss = epoch\n            self.best_loss_model = copy.deepcopy(model.state_dict())\n\n        # Update best PM model\n        if pm > self.best_pm:\n            self.best_pm = pm\n            self.best_run_pm = run\n            self.best_epoch_pm = epoch\n            self.best_pm_model = copy.deepcopy(model.state_dict())\n\n    def save_models(self, dataname):\n        torch.save(self.best_f1_model, f'model-f1-{dataname}-{self.best_run_f1}-{self.best_epoch_f1}.pt')\n        torch.save(self.best_loss_model, f'model-loss-{dataname}-{self.best_run_loss}-{self.best_epoch_loss}.pt')\n        torch.save(self.best_pm_model, f'model-pm-{dataname}-{self.best_run_pm}-{self.best_epoch_pm}.pt')\n            \ndef get_class_weights(df, config):\n    \n    labels = df.labels.values\n    labels = [t.split() for t in labels]\n    labels = [j for sub in labels for j in sub]\n    total_samples = len(labels)\n    \n    mapping = config.label_list\n    labels = [mapping.index(x) for x in labels]\n    class_counts = torch.bincount(torch.tensor(labels))\n    class_weights = total_samples / (len(mapping) * class_counts)\n    class_weights = class_weights / class_weights.sum()\n    class_weights = [round(weight, 4) for weight in class_weights.cpu().numpy()]\n    \n    return class_weights\n\ndef tokenize_and_align_labels(txts, lbls, config):\n    tokenizer, max_len, mapping = config.tokenizer, config.max_len, config.label_list\n\n    tokenized_inputs = tokenizer(txts, is_split_into_words=True,\n                                 max_length = max_len, \n                                 padding = 'max_length', \n                                 truncation=True,\n                                 return_tensors = 'pt')\n\n    labels = []\n    for i, label in enumerate(lbls):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        previous_label = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.            \n            if word_idx is None:\n                label_ids.append(-100)\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(mapping.index(label[word_idx]))\n                previous_label = label[word_idx]\n            # For the other tokens in a word, we set the label to the current label.\n            else:\n                new_label = 'O' if previous_label == 'O' else 'I'+previous_label[1:]\n                label_ids.append(mapping.index(new_label))\n                previous_label = new_label\n                \n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    return tokenized_inputs, labels\n\nclass SequenceLabelingDataset(torch.utils.data.Dataset):\n    def __init__(self, df, config):\n        lb = [x.split() for x in df.labels.values.tolist()]\n        txt = [i.split() for i in df.tokens.values.tolist()]\n        self.encodings, self.labels = tokenize_and_align_labels(txt, lb, config)\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n    \nclass SimpleTagger(nn.Module):\n    def __init__(self, config):\n        super(SimpleTagger, self).__init__()\n        self.configuration = config\n        self.transf = AutoModelForTokenClassification.from_pretrained(config.model_checkpoint, num_labels = config.num_labels)\n    \n    def forward(self, input_ids, attention_mask, labels = None, lossfn = None):\n        \n        if labels is not None: # training\n            \n            output_hidden = (self.configuration.use_similarity) and (self.configuration.mode_similarity == 'cosine')\n            \n            outputs = self.transf(input_ids = input_ids, \n                                attention_mask = attention_mask, \n                                labels = labels, \n                                output_hidden_states = output_hidden)\n            \n            loss = lossfn(outputs.logits.view(-1, 3), labels.view(-1).long())\n            \n            if output_hidden:\n                return loss, outputs.logits, outputs.hidden_states[-1]\n            else:\n                return loss, outputs.logits, None\n        else: # inference\n            outputs = self.transf(input_ids = input_ids, \n                                    attention_mask = attention_mask)\n            return outputs.logits\n        \ndef load_data(df, config):\n    \n    train_seq_df = df.loc[df['set'] == 'train']\n    if 'dev' in df['set'].values:\n        val_seq_df = df.loc[df['set'] == 'dev']\n    else:\n        train_seq_df, val_seq_df = train_test_split(train_seq_df, test_size = 0.1, random_state = 2023)\n    test_seq_df = df.loc[df['set'] == 'test']\n\n    train_dataset, val_dataset, test_dataset = SequenceLabelingDataset(train_seq_df, config), \\\n                                            SequenceLabelingDataset(val_seq_df, config), \\\n                                            SequenceLabelingDataset(test_seq_df, config)\n    \n    batch_size = config.batch_size\n    train_loader, val_loader, test_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True), \\\n                                            DataLoader(val_dataset, batch_size=batch_size), \\\n                                            DataLoader(test_dataset, batch_size=batch_size)\n    \n    class_weights = get_class_weights(train_seq_df, config)\n    \n    return train_loader, val_loader, test_loader, class_weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_arguments_indices(sequence):\n    indices = []\n    start_index = None\n    for i, label in enumerate(sequence):\n        if label == 1:\n            \n            if start_index is None:\n                start_index = i\n            else:\n                indices.append((start_index, i))\n                start_index = i\n        \n        elif (label == 0) and (start_index is not None):\n            indices.append((start_index, i))\n            start_index = None\n\n    if start_index is not None:\n        indices.append((start_index, len(sequence)))\n\n    return indices\n\ndef compute_r_matrix(gold_indices, predicted_indices):\n\n    def get_R(gold_argument, predicted_argument):\n        (gold_start, gold_end) = gold_argument\n        (pred_start, pred_end) = predicted_argument\n\n        intersection_start = max(gold_start, pred_start)\n        intersection_end = min(gold_end, pred_end)\n\n        len_intersection_interval = (intersection_end - intersection_start) if intersection_start <= intersection_end else 0\n        len_longer_span = max(gold_end - gold_start, pred_end - pred_start)\n        return round((len_intersection_interval / len_longer_span), 3)\n    \n    R_matrix = np.zeros((len(gold_indices), len(predicted_indices)), dtype=float)\n    \n    for i, gold_argument in enumerate(gold_indices):\n        for j, predicted_argument in enumerate(predicted_indices):\n            R_matrix[i][j] = get_R(gold_argument, predicted_argument)\n    \n    return R_matrix\n\ndef get_arguments_from_tokens(tokens, tokenizer):\n    return tokenizer.convert_tokens_to_string(tokens).strip()\n\n\ndef get_categorized_arguments(clean, config):\n    instances = [(x, y, z) for x, y, z in zip(clean[0], clean[1], clean[2])]\n    \n    categorization = []\n    nro_unrecognized = 0\n    \n    for (tokens, labels, predictions) in instances:\n        ground_truth_indices = get_arguments_indices(labels)\n        ground_truth_arguments = [get_arguments_from_tokens(tokens[gr_start : gr_end], config.tokenizer) for (gr_start, gr_end) in ground_truth_indices]\n        \n        predicted_args_indices = get_arguments_indices(predictions)\n        \n        R_matrix = compute_r_matrix(ground_truth_indices, predicted_args_indices)\n        \n        for i, (pr_start, pr_end) in enumerate(predicted_args_indices):\n            column = R_matrix[:, i]\n            positive_values = column[column > 0].tolist()\n            predicted_argument = get_arguments_from_tokens(tokens[pr_start:pr_end], config.tokenizer)\n            \n            if len(positive_values) > 0:\n                \n                for index, val in enumerate(column):\n                    if val > 0:\n                        if val == 1:\n                            categorization.append((predicted_argument, ground_truth_arguments[index], 'Match'))\n                        else:\n                            categorization.append((predicted_argument, ground_truth_arguments[index], 'no-Match'))\n            else:\n                categorization.append((predicted_argument, '-', 'made-up'))\n                \n        for i, (gd_start, gd_end) in enumerate(ground_truth_indices):\n            row = R_matrix[i, :]\n            positive_values = row[row > 0].tolist()\n            if len(positive_values) == 0:\n                nro_unrecognized += 1\n            \n\n    return categorization, nro_unrecognized","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, tokenizer):\n    eval_tokens = []\n    eval_labels = []\n    eval_predictions = []\n    for i in range(len(batch_input_ids)):\n        tokens = tokenizer.convert_ids_to_tokens(batch_input_ids[i].tolist())\n        labels = batch_labels[i]\n        preds = batch_predictions[i]\n        \n        if '<s>' in tokens and '</s>' in tokens:\n            start = tokens.index('<s>')\n            end = tokens.index('</s>') + 1\n            tokens = tokens[start:end]\n            labels = labels[start:end]\n            preds = preds[start:end]\n            \n        assert len(preds) == len(tokens)\n        \n        \n        filtered_tokens, filtered_labels, filtered_preds = [], [], []\n        for tk, lb, pr in zip(tokens, labels, preds):\n            if tk not in tokenizer.all_special_tokens:\n#                 tk = tk.lstrip('Ġ')\n#                 if tk != '':\n                if tk != '' and tk != '<s>' and tk != '</s>':\n                    filtered_tokens.append(tk)\n                    filtered_labels.append(lb)\n                    filtered_preds.append(pr)\n                \n        eval_tokens.append(filtered_tokens)\n        eval_labels.append(filtered_labels)\n        eval_predictions.append(filtered_preds)\n        \n    return eval_tokens, eval_labels, eval_predictions\n\n            \ndef fn_similarity_1(golds, preds, config):\n    # Compute embeddings for both lists\n    embeddings1 = config.similarity_model.encode(golds, show_progress_bar=False)\n    embeddings2 = config.similarity_model.encode(preds, show_progress_bar=False)\n\n    similarities = config.similarity_model.similarity(embeddings1, embeddings2)\n    return [(1-abs(similarities[i][i].item())) for i in range(len(similarities))]\n\n\ndef count_predicted_arguments(clean, config):\n    categorized_arguments, _ = get_categorized_arguments(clean, config)\n    \n    number_predicted = len(categorized_arguments)\n    number_pm_predicted = len([x[0] for x in categorized_arguments if x[2] == 'Match'])\n    number_nomatch_predicted = len([x[0] for x in categorized_arguments if x[2] == 'no-Match'])\n    number_madeup_predicted = len([x[0] for x in categorized_arguments if x[2] == 'made-up'])\n\n    return number_predicted, number_pm_predicted, number_nomatch_predicted, number_madeup_predicted\n\ndef count_gold_arguments(clean, config):\n    instances = [(x, y, z) for x, y, z in zip(clean[0], clean[1], clean[2])]\n    \n    number_gold_arguments = 0\n    \n    categorized_arguments, nro_unrecognized = get_categorized_arguments(clean, config)\n    \n    for (tokens, labels, predictions) in instances:\n        ground_truth_indices = get_arguments_indices(labels)\n        number_gold_arguments += len(ground_truth_indices)\n        \n    return number_gold_arguments, nro_unrecognized\n    \ndef compute_similarity_error(clean, config):\n    partial_match_error = 0\n    made_up_error = 0\n    unrecognized_error = 0\n    \n    categorized_arguments, number_unrecognized_arguments = get_categorized_arguments(clean, config)    \n    no_match_arguments = [(x[0], x[1]) for x in categorized_arguments if x[2] == 'no-Match']\n    madeup_arguments = [x[0] for x in categorized_arguments if x[2] == 'made-up']\n    \n    number_predicted_arguments = len(categorized_arguments)\n    number_gold_arguments, _ = count_gold_arguments(clean, config)\n    \n    if len(no_match_arguments) > 0:\n        pred_args = [x for x, y in no_match_arguments]\n        gold_args = [y for x, y in no_match_arguments]\n        partial_match_error = round(sum(fn_similarity_1(gold_args, pred_args, config)), 3)\n        \n    epsilon = 1e-10\n    made_up_error = len(madeup_arguments) / (number_predicted_arguments + epsilon)\n    \n    unrecognized_error = number_unrecognized_arguments / (number_gold_arguments + epsilon)\n    \n    total_similarity_error = partial_match_error + config.lambda_mu*made_up_error + config.lambda_unr*unrecognized_error\n    return total_similarity_error\n\n\ndef train_model(model, train_loader, optimizer, config, lossfn):\n#     progress_bar = tqdm(range(len(train_loader)))\n    \n    model.train()\n\n    train_loss = 0\n    \n    for batch in train_loader:\n        batch = tuple(v.to(device) for t, v in batch.items())\n        loss, outputs = None, None\n        \n        batch_input_ids, batch_attention_mask, batch_labels = batch\n        loss, outputs, last_hidden_states = model(batch_input_ids, attention_mask = batch_attention_mask, labels = batch_labels, lossfn = lossfn)\n                        \n        if config.use_similarity:\n            batch_labels = batch_labels.detach().cpu().numpy()\n            batch_predictions = np.argmax(outputs.detach().cpu().numpy(), axis = 2).tolist()\n            clean_elements = clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, config.tokenizer)\n            similarity_loss = compute_similarity_error(clean_elements, config)\n            \n            loss = loss + similarity_loss\n            \n        train_loss += loss.item()\n\n        # backprop\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        \n        print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1024**2} MB\")\n        print(f\"Total GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2} MB\")\n        \n#         progress_bar.update(1)\n\n    return round((train_loss / len(train_loader)), 4)\n\ndef evaluate_model(model, dataloader, config, tokenizer, lossfn):\n\n    model.eval()\n\n    eval_loss = 0\n    number_of_predicted_arguments  = 0\n    number_of_predicted_pm_arguments = 0\n    number_of_predicted_nomatch_arguments = 0\n    number_of_predicted_madeup_arguments = 0\n    number_of_gold_arguments = 0\n    number_of_unrecognized_arguments = 0\n    \n    eval_tokens, eval_labels, eval_predictions = [], [], []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            batch = tuple(v.to(device) for t, v in batch.items())\n            loss, outputs = None, None\n            \n            batch_input_ids, batch_attention_mask, batch_labels = batch\n            loss, outputs, _ = model(batch_input_ids, attention_mask = batch_attention_mask, labels = batch_labels, lossfn = lossfn)\n            \n            batch_labels = batch_labels.detach().cpu().numpy()\n            batch_predictions = np.argmax(outputs.detach().cpu().numpy(), axis = 2).tolist()\n            clean_elements = clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, tokenizer)\n            \n            batch_predicted_arguments_counts = count_predicted_arguments(clean_elements, config)\n            batch_gold_arguments_counts = count_gold_arguments(clean_elements, config)\n            \n            number_of_predicted_arguments += batch_predicted_arguments_counts[0]\n            number_of_predicted_pm_arguments += batch_predicted_arguments_counts[1]\n            number_of_predicted_nomatch_arguments += batch_predicted_arguments_counts[2]\n            number_of_predicted_madeup_arguments += batch_predicted_arguments_counts[3]\n            \n            number_of_gold_arguments += batch_gold_arguments_counts[0]\n            number_of_unrecognized_arguments += batch_gold_arguments_counts[1]\n            \n            if config.use_similarity:\n                similarity_loss = compute_similarity_error(clean_elements, config)\n                loss = loss + similarity_loss\n\n            eval_loss += loss.item()\n            \n            eval_tokens += clean_elements[0]\n            eval_labels += clean_elements[1]\n            eval_predictions += clean_elements[2]\n            \n#             print(f\"GPU memory used: {torch.cuda.memory_allocated() / 1024**2} MB\")\n#             print(f\"Total GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2} MB\")\n\n            \n    \n    flattened_labels = [j for sub in eval_labels for j in sub]\n    flattened_predictions = [j for sub in eval_predictions for j in sub]\n    \n    eval_loss = round((eval_loss / len(dataloader)), 4)\n    eval_f1 = f1_score(flattened_labels, flattened_predictions, average = 'macro')\n    eval_pm = round((number_of_predicted_pm_arguments / number_of_predicted_arguments), 4)\n    eval_counter = [number_of_predicted_pm_arguments, \n                    number_of_predicted_nomatch_arguments, \n                    number_of_predicted_madeup_arguments, \n                    number_of_predicted_arguments,\n                    number_of_unrecognized_arguments,\n                    number_of_gold_arguments]\n    \n    return eval_loss, eval_f1, eval_pm, eval_counter\n\n\ndef test_model(model, dataloader, tokenizer):\n\n    model.eval()\n\n    eval_tokens, eval_labels, eval_predictions = [], [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            batch = tuple(v.to(device) for t, v in batch.items())\n            loss, outputs = None, None\n            \n            batch_input_ids, batch_attention_mask, batch_labels = batch\n            logits = model(batch_input_ids, attention_mask = batch_attention_mask)\n            \n            batch_labels = batch_labels.detach().cpu().numpy()\n            batch_predictions = np.argmax(logits.detach().cpu().numpy(), axis = 2).tolist()\n            \n            clean_elements = clean_batch_elements(batch_input_ids, batch_labels, batch_predictions, tokenizer)\n            eval_tokens += clean_elements[0]\n            eval_labels += clean_elements[1]\n            eval_predictions += clean_elements[2]\n    \n    return eval_tokens, eval_labels, eval_predictions\n\ndef save_predictions(tokens, labels, predictions, file_path):\n    with open(file_path, 'w', encoding = 'utf-8') as nf:\n\n        for tks, lbs, prds in zip(tokens, labels, predictions):\n            for tk, lb, pr in zip(tks, lbs, prds):\n                nf.write(f\"{tk} {lb} {pr}\\n\")\n\n            nf.write(f\"\\n\") ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# DATA\ndataname = 'pe' \n\ndf = pd.read_csv(f'/kaggle/input/mtaa-proyecto-final/{dataname}.csv')\nprint(df.shape)\n\nsimilarity_modes = ['sbert', 'sbert-sts-bws', 'arguebert-sts-bws']\n\n# lambda_madeup = 1, lambda_unrecognized = 1 webis\n# lambda_madeup = 10, lambda_unrecognized = 1 ugen\n# lambda_madeup = 1, lambda_unrecognized = 10 pe\n\n# CONFIG\nexperiments_configuration = Config(runs = 10, epochs = 15, batch_size = 4, max_len = 1024, \n                                   model_checkpoint = \"allenai/longformer-base-4096\", \n                                   num_labels = 3, label_list = ['O', 'B', 'I'], \n                                   use_similarity = True, mode_similarity = similarity_modes[2],\n                                   lambda_madeup = 1, lambda_unrecognized = 10)\n\ntracker = BestModelsTracker() # tracker for all runs.\ncounter_data = []\n\n\nfor nrun in range(experiments_configuration.runs):\n    rs = generate_random_seed()\n    set_random_seed(rs)\n    \n    run_best_eval_loss = float('inf')\n    run_best_epoch = 0\n    run_best_model_state = None\n    \n    # MODEL\n    tagger = SimpleTagger(experiments_configuration)\n    tagger.to(device)\n\n    # OPTIMIZER\n    optimizer = torch.optim.AdamW(tagger.parameters(), lr = 1e-5, eps = 1e-8)\n    \n    train_loader, val_loader, test_loader, class_weights = load_data(df, experiments_configuration)\n    print(len(train_loader), len(val_loader), len(test_loader))\n        \n    base_loss_fn = nn.CrossEntropyLoss(weight = torch.tensor(class_weights, dtype=torch.float32).to(device)) if dataname == 'ugen' else nn.CrossEntropyLoss()\n        \n    \n    for epoch in range(experiments_configuration.epochs):\n        \n        train_loss = train_model(tagger, train_loader, optimizer, experiments_configuration, base_loss_fn)\n        \n        # evaluate model\n        eval_loss, eval_f1, eval_pm, eval_counter = evaluate_model(tagger, val_loader, experiments_configuration, experiments_configuration.tokenizer, base_loss_fn)\n        counter_data.append([dataname, nrun, epoch]+eval_counter)\n        print(f\"Epoch: {epoch}/{experiments_configuration.epochs-1} | Train_loss={train_loss} | Eval_loss={eval_loss} | Eval_F1={eval_f1} | Eval_PM={eval_pm}\")\n        \n        tracker.update(tagger, eval_loss, eval_f1, eval_pm, nrun, epoch)\n        \n        if eval_loss < run_best_eval_loss:\n            run_best_eval_loss = eval_loss\n            run_best_epoch = epoch\n            run_best_model_state = copy.deepcopy(tagger.state_dict())\n            print(f\"New best model in epoch {epoch} (eval_loss = {eval_loss})\")\n            \n    # testing\n    best_tagger = SimpleTagger(experiments_configuration)\n    best_tagger.load_state_dict(run_best_model_state)\n    best_tagger.to(device)\n    \n    tokens, labels, preds = test_model(best_tagger, test_loader, experiments_configuration.tokenizer)\n    assert len(tokens) == len(labels) == len(preds)\n    if len(tokens) > 0:\n        assert len(tokens[0]) == len(labels[0]) == len(preds[0])\n\n    save_predictions(tokens, labels, preds, f'test_{dataname}_{nrun}.txt')\n    \n#     if run_best_eval_loss < best_loss:\n#         best_loss = run_best_eval_loss\n#         best_model = copy.deepcopy(run_best_model_state)\n#         best_run = nrun\n        \n    print(f\"end run {nrun}\")\n    print()\n    \n    \n# saving best model\n# model_path = f\"model-{best_run}-{dataname}.pt\"\n# if best_model is not None:\n#     torch.save(best_model, model_path)\n\ntracker.save_models(dataname)\npd.DataFrame(counter_data, columns = ['dataname', 'run', 'epoch', 'pm', 'no-pm', 'mu', 'total_predicted', 'unr', 'total_gold']).to_csv(f'errors_{dataname}.csv', index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\nfrom IPython.display import FileLink\n\ndef zip_files(folder_path, zip_name):\n    # Crear un archivo ZIP\n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Recorrer todos los archivos en la carpeta\n        for foldername, subfolders, filenames in os.walk(folder_path):\n            for filename in filenames:\n                # Comprobar si el archivo es un archivo TXT o CSV\n                if filename.endswith('.txt') or filename.endswith('.csv') or filename.endswith('.pt'):\n                    # Ruta completa del archivo\n                    file_path = os.path.join(foldername, filename)\n                    # Agregar el archivo al archivo ZIP\n                    zipf.write(file_path, os.path.relpath(file_path, folder_path))\n\n# Llamar a la función para comprimir los archivos\nfolder_path = '/kaggle/working/'\nzip_name = 'test.zip'\nzip_files(folder_path, zip_name)\n\nFileLink(r'test.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}