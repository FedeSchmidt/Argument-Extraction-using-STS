{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install sentence-transformers datasets"
      ],
      "metadata": {
        "id": "jfXfI8ki8pX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer, losses, models\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.similarity_functions import SimilarityFunction\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "\n",
        "modelpath = '/content/drive/MyDrive/Doctorado/Beca Doctoral Fede Schmidt/Proyectos/Semantic similarity & arguments identification/'\n",
        "datapath = '/content/drive/MyDrive/Doctorado/Beca Doctoral Fede Schmidt/Proyectos/Semantic similarity & arguments identification/Datasets/'\n",
        "\n",
        "model_names = [\"sentence-transformers/all-MiniLM-L6-v2\", modelpath + 'argueBert_base_similar/', modelpath + 'argueBert_edge/']\n",
        "short_model_names = [\"sbert-all-MiniLM-L6-v2\", 'argueBert_base_similar', 'argueBert_edge']\n",
        "\n",
        "index_selected_model = 2    ## change here.\n",
        "\n",
        "model_name = model_names[index_selected_model]\n",
        "short_model_name = short_model_names[index_selected_model]\n",
        "\n",
        "def build_model(model_name):\n",
        "    model = None\n",
        "    if model_name == \"sentence-transformers/all-MiniLM-L6-v2\":\n",
        "        model = SentenceTransformer(model_name)\n",
        "    else:\n",
        "        word_embedding_model = models.Transformer(model_name)\n",
        "        # print(word_embedding_model)\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        # print(pooling_model)\n",
        "        normalize_layer = models.Normalize()\n",
        "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model, normalize_layer])\n",
        "\n",
        "    for param in model.parameters(): param.data = param.data.contiguous()\n",
        "    return model\n",
        "\n",
        "\n",
        "# model\n",
        "model = build_model(model_name)\n",
        "\n",
        "print()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "1DxKIn7ktY2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85lkgLo58f07"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Code based on https://github.com/mabehrendt/argueBERT\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer, losses, models\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.similarity_functions import SimilarityFunction\n",
        "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
        "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
        "\n",
        "\n",
        "modelpath = '/content/drive/MyDrive/Doctorado/Beca Doctoral Fede Schmidt/Proyectos/Semantic similarity & arguments identification/'\n",
        "datapath = '/content/drive/MyDrive/Doctorado/Beca Doctoral Fede Schmidt/Proyectos/Semantic similarity & arguments identification/Datasets/'\n",
        "\n",
        "model_names = [\"sentence-transformers/all-MiniLM-L6-v2\", modelpath + 'argueBert_base_similar/', modelpath + 'argueBert_edge/']\n",
        "short_model_names = [\"sbert-all-MiniLM-L6-v2\", 'argueBert_base_similar', 'argueBert_edge']\n",
        "\n",
        "index_selected_model = 0    ## change here.\n",
        "\n",
        "model_name = model_names[index_selected_model]\n",
        "short_model_name = short_model_names[index_selected_model]\n",
        "\n",
        "# hyperparams of argueBERT for fine-tuning.\n",
        "train_batch_size = 16\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "optimizer = \"adamw_hf\"\n",
        "\n",
        "do_fine_tuning = False ## change here\n",
        "\n",
        "# build model\n",
        "def build_model(model_name):\n",
        "    model = None\n",
        "    if model_name == \"sentence-transformers/all-MiniLM-L6-v2\":\n",
        "        model = SentenceTransformer(model_name)\n",
        "    else:\n",
        "        word_embedding_model = models.Transformer(model_name)\n",
        "        pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "        normalize_layer = models.Normalize()\n",
        "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model, normalize_layer])\n",
        "\n",
        "    for param in model.parameters(): param.data = param.data.contiguous()\n",
        "    return model\n",
        "\n",
        "def load_data(path = 'STSb/'):\n",
        "    dataset_path = datapath + path\n",
        "    if 'STSb' in path:\n",
        "        train_dataset = load_dataset('parquet', data_files=dataset_path+'train.parquet')\n",
        "        eval_dataset = load_dataset('parquet', data_files=dataset_path+'validation.parquet')\n",
        "        test_dataset = load_dataset('parquet', data_files=dataset_path+'test.parquet')\n",
        "        train_dataset = train_dataset[\"train\"]\n",
        "        eval_dataset = eval_dataset[\"train\"]\n",
        "        test_dataset = test_dataset[\"train\"]\n",
        "    elif 'BWS' in path:\n",
        "        train_dataset = load_dataset(\"csv\", data_files=dataset_path+\"trainsplit.csv\")\n",
        "        eval_dataset = load_dataset(\"csv\", data_files=dataset_path+\"devsplit.csv\")\n",
        "        test_dataset = load_dataset(\"csv\", data_files=dataset_path+\"testsplit.csv\")\n",
        "\n",
        "        train_dataset = train_dataset[\"train\"].remove_columns(['id', 'topic'])\n",
        "        train_dataset = train_dataset.rename_column(\"argument1\", \"sentence1\")\n",
        "        train_dataset = train_dataset.rename_column(\"argument2\", \"sentence2\")\n",
        "\n",
        "        eval_dataset = eval_dataset[\"train\"].remove_columns(['id', 'topic'])\n",
        "        eval_dataset = eval_dataset.rename_column(\"argument1\", \"sentence1\")\n",
        "        eval_dataset = eval_dataset.rename_column(\"argument2\", \"sentence2\")\n",
        "\n",
        "        test_dataset = test_dataset[\"train\"].remove_columns(['id', 'topic'])\n",
        "        test_dataset = test_dataset.rename_column(\"argument1\", \"sentence1\")\n",
        "        test_dataset = test_dataset.rename_column(\"argument2\", \"sentence2\")\n",
        "\n",
        "    elif 'AFS' in path:\n",
        "        train_dataset = load_dataset(\"csv\", data_files=dataset_path+\"trainsplit.csv\")\n",
        "        eval_dataset = load_dataset(\"csv\", data_files=dataset_path+\"devsplit.csv\")\n",
        "        test_dataset = load_dataset(\"csv\", data_files=dataset_path+\"testsplit.csv\")\n",
        "\n",
        "        train_dataset = train_dataset[\"train\"]\n",
        "        train_dataset = train_dataset.rename_column(\"sentence_1\", \"sentence1\")\n",
        "        train_dataset = train_dataset.rename_column(\"sentence_2\", \"sentence2\")\n",
        "\n",
        "        eval_dataset = eval_dataset[\"train\"]\n",
        "        eval_dataset = eval_dataset.rename_column(\"sentence_1\", \"sentence1\")\n",
        "        eval_dataset = eval_dataset.rename_column(\"sentence_2\", \"sentence2\")\n",
        "\n",
        "        test_dataset = test_dataset[\"train\"]\n",
        "        test_dataset = test_dataset.rename_column(\"sentence_1\", \"sentence1\")\n",
        "        test_dataset = test_dataset.rename_column(\"sentence_2\", \"sentence2\")\n",
        "    else:\n",
        "        raise Exception(\"No dataset in given path...\")\n",
        "\n",
        "    return (train_dataset, eval_dataset, test_dataset)\n",
        "\n",
        "\n",
        "output_dir = (\n",
        "    \"output/training_stsbenchmark_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        ")\n",
        "\n",
        "# model\n",
        "model = build_model(model_name)\n",
        "print(model)\n",
        "\n",
        "# data\n",
        "sts_data = load_data()\n",
        "bws_data = load_data('BWS Argument Similarity/')\n",
        "afs_data = load_data('AFS/')\n",
        "\n",
        "\n",
        "# train_dataset = concatenate_datasets([sts_data[0]])\n",
        "train_dataset = concatenate_datasets([bws_data[0]])\n",
        "# train_dataset = concatenate_datasets([afs_data[0]])\n",
        "# train_dataset = concatenate_datasets([sts_data[0], bws_data[0]])\n",
        "# train_dataset = concatenate_datasets([sts_data[0], afs_data[0]])\n",
        "# train_dataset = concatenate_datasets([sts_data[0], bws_data[0], afs_data[0]])\n",
        "\n",
        "# eval_dataset = concatenate_datasets([sts_data[1]])\n",
        "eval_dataset = concatenate_datasets([bws_data[1]])\n",
        "# eval_dataset = concatenate_datasets([afs_data[1]])\n",
        "# eval_dataset = concatenate_datasets([sts_data[1], bws_data[1]])\n",
        "# eval_dataset = concatenate_datasets([sts_data[1], afs_data[1]])\n",
        "# eval_dataset = concatenate_datasets([sts_data[1], bws_data[1], afs_data[1]])\n",
        "\n",
        "test_datasets = [sts_data[2], bws_data[2], afs_data[2]]\n",
        "\n",
        "# Define training loss\n",
        "train_loss = losses.CosineSimilarityLoss(model=model)\n",
        "\n",
        "# Define an evaluator for use during training. This is useful to keep track of alongside the evaluation loss.\n",
        "dev_evaluator = EmbeddingSimilarityEvaluator(\n",
        "    sentences1=eval_dataset[\"sentence1\"],\n",
        "    sentences2=eval_dataset[\"sentence2\"],\n",
        "    scores=eval_dataset[\"score\"],\n",
        "    main_similarity=SimilarityFunction.COSINE,\n",
        "    name=\"res-dev\",\n",
        ")\n",
        "\n",
        "# Define the training arguments\n",
        "args = SentenceTransformerTrainingArguments(\n",
        "    # Required parameter:\n",
        "    output_dir=output_dir,\n",
        "    # Optional training parameters:\n",
        "    num_train_epochs=num_epochs,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=train_batch_size,\n",
        "    warmup_ratio=0.1,\n",
        "    optim = optimizer,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=False,  # Set to False if you get an error that your GPU can't run on FP16\n",
        "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
        "    # Optional tracking/debugging parameters:\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_safetensors=False,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"steps\",\n",
        "    # save_strategy=\"no\",\n",
        "    logging_steps=100\n",
        ")\n",
        "\n",
        "# Create the trainer & start training\n",
        "trainer = SentenceTransformerTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss=train_loss,\n",
        "    evaluator=dev_evaluator,\n",
        ")\n",
        "\n",
        "if do_fine_tuning:\n",
        "    trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_data = []\n",
        "for test_dataset, test_dataset_name in zip(test_datasets, ['STS', 'BWS', 'AFS']):\n",
        "    test_evaluator = EmbeddingSimilarityEvaluator(\n",
        "        sentences1=test_dataset[\"sentence1\"],\n",
        "        sentences2=test_dataset[\"sentence2\"],\n",
        "        scores=test_dataset[\"score\"],\n",
        "        main_similarity=SimilarityFunction.COSINE,\n",
        "        name=\"test\",\n",
        "    )\n",
        "\n",
        "    results = test_evaluator(model)\n",
        "    results['test'] = test_dataset_name\n",
        "\n",
        "    results_data.append(results)\n",
        "\n",
        "\n",
        "# save testing results\n",
        "results_dataframe = pd.DataFrame(results_data)\n",
        "results_dataframe"
      ],
      "metadata": {
        "id": "2mWWJ_T9M7IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dataframe.to_csv(f\"test_results_{'sbert_no_ft'}.csv\")"
      ],
      "metadata": {
        "id": "6oxrUOwlgT2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save trained model\n",
        "\n",
        "# names_train_sets = 'sts' ## change here based on used training set.\n",
        "# names_train_sets = 'sts-bws'\n",
        "# names_train_sets = 'sts-afs'\n",
        "# names_train_sets = 'sts-bws-afs'\n",
        "# names_train_sets = 'bws'\n",
        "names_train_sets = 'afs'\n",
        "saving_model_name = short_model_name + '-' + names_train_sets\n",
        "\n",
        "model.save_pretrained(f\"models/{saving_model_name}/final\")\n",
        "\n",
        "results_dataframe.to_csv(f\"models/{saving_model_name}/final/test_results_{short_model_name}.csv\")"
      ],
      "metadata": {
        "id": "ncuNF_4mt509"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/models/sbert-all-MiniLM-L6-v2-sts-afs\n",
        "# /content/models/sbert-all-MiniLM-L6-v2-sts-bws-afs\n",
        "# /content/models/argueBert_base_similar-sts\n",
        "# /content/models/argueBert_base_similar-bws\n",
        "\n",
        "!zip -r /content/models/argueBert_base_similar-afs/arguebert-afs.zip /content/models/argueBert_base_similar-afs/final\n",
        "\n",
        "!cp /content/models/argueBert_base_similar-afs/arguebert-afs.zip '/content/drive/MyDrive/Doctorado/Beca Doctoral Fede Schmidt/Proyectos/Semantic similarity & arguments identification/similarity_models'"
      ],
      "metadata": {
        "id": "frKePjKjuue_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USAGE"
      ],
      "metadata": {
        "id": "oc23F7IGPNkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trained_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "# trained_model = SentenceTransformer('/content/models/all-MiniLM-L6-v2-stsb/final')\n",
        "# trained_model = SentenceTransformer('/content/models/argueBert_similar-stsb/final')\n",
        "trained_model = SentenceTransformer('/content/models/argueBert_base_similar-stsb/final')\n",
        "\n",
        "trained_model"
      ],
      "metadata": {
        "id": "ltcHC-6VPPLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The weather is lovely today.\",\n",
        "    \"It's so sunny outside!\",\n",
        "    \"He drove to the stadium.\",\n",
        "]\n",
        "\n",
        "# 2. Calculate embeddings by calling model.encode()\n",
        "embeddings = trained_model.encode(sentences)\n",
        "print(embeddings.shape)\n",
        "# [3, 384]\n",
        "\n",
        "# 3. Calculate the embedding similarities\n",
        "similarities = trained_model.similarity(embeddings, embeddings)\n",
        "print(similarities)\n",
        "# tensor([[1.0000, 0.6660, 0.1046],\n",
        "#         [0.6660, 1.0000, 0.1411],\n",
        "#         [0.1046, 0.1411, 1.0000]])"
      ],
      "metadata": {
        "id": "U-wOEHjePmDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}